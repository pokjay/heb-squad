{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune_alephbert_squad_new_dataset",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMb+PkaXHcafw3wyJK5NrUe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c2bb3d95363048688c0962ba9fc57bfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0b4579d3c3f1479ebc09c9c1b8382abb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b3f13e240ed140b3a98f729c6d932b94",
              "IPY_MODEL_ac8100de07f2482f878c61e10a81ef39",
              "IPY_MODEL_caaa2acfaab448b3aa9ff42166a735f3"
            ]
          }
        },
        "0b4579d3c3f1479ebc09c9c1b8382abb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b3f13e240ed140b3a98f729c6d932b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b934bfce3a804dbe9ae97554da9bfc89",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fe0b6e3ec1354f4f91e68ca23806d87e"
          }
        },
        "ac8100de07f2482f878c61e10a81ef39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_63629e10a88e429f82ce82ef613845f5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1aebb6e2b899489a9bed56306fa63687"
          }
        },
        "caaa2acfaab448b3aa9ff42166a735f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8c15c6da8f7c4d619dc1473c28213cde",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1413/? [06:56&lt;00:00,  3.78it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e5576954f0e54e2dbe0dd19669a26827"
          }
        },
        "b934bfce3a804dbe9ae97554da9bfc89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fe0b6e3ec1354f4f91e68ca23806d87e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "63629e10a88e429f82ce82ef613845f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1aebb6e2b899489a9bed56306fa63687": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": "20px",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8c15c6da8f7c4d619dc1473c28213cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e5576954f0e54e2dbe0dd19669a26827": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "080479a8ee93416cbe8d0293907ba9bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_83f9caa4792f4bb4b9d76d2d41073535",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_36374d9b6b76494d8551a8d0545fff75",
              "IPY_MODEL_935c8b070cd64a94961cf55008a9c31a",
              "IPY_MODEL_599dde8bf87247ad9d2ee85ff7ab4482"
            ]
          }
        },
        "83f9caa4792f4bb4b9d76d2d41073535": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "36374d9b6b76494d8551a8d0545fff75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1a1777c285a549fe87d2a65453b49fc8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9a9f475b56a942f6bcb0307765e47e58"
          }
        },
        "935c8b070cd64a94961cf55008a9c31a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_096870196dcc471b8ae95b73a538efd5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e724a7a570543988b7052525113ab95"
          }
        },
        "599dde8bf87247ad9d2ee85ff7ab4482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_73d9846f1923405cb2ff9d82b97a89cf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1413/? [06:56&lt;00:00,  3.77it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_07f112bdfff2415d866596eb2deca0e9"
          }
        },
        "1a1777c285a549fe87d2a65453b49fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9a9f475b56a942f6bcb0307765e47e58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "096870196dcc471b8ae95b73a538efd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e724a7a570543988b7052525113ab95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": "20px",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "73d9846f1923405cb2ff9d82b97a89cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "07f112bdfff2415d866596eb2deca0e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pokjay/heb-squad/blob/main/finetune_alephbert_squad_new_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaVaSmmPLtlz",
        "outputId": "11d91d5c-8b66-4354-ee18-d627ed699756"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Aug 23 14:08:00 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3d5WB9mwvM8"
      },
      "source": [
        "## Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pH32Mr4wuh_",
        "outputId": "3e897ad3-bc5c-41e9-b0e6-b6410a617ba3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1cT6F9cpbnv",
        "outputId": "d444e285-4d8e-44a7-dcf7-51d213340cd5"
      },
      "source": [
        "!git clone https://github.com/pokjay/heb-squad"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'heb-squad' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN8DXFYoxoz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86c0c2b3-4f0c-42e2-987d-e67ca33f77bf"
      },
      "source": [
        "!gzip -d /content/heb-squad/data/final/heb-train-v2.0.csv.gz\n",
        "!gzip -d /content/heb-squad/data/final/heb-dev-v2.0.csv.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gzip: /content/heb-squad/data/final/heb-train-v2.0.csv.gz: No such file or directory\n",
            "gzip: /content/heb-squad/data/final/heb-dev-v2.0.csv.gz: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_06SF-Vz8ao",
        "outputId": "615b674c-8289-4897-e670-03a5ddb2008a"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMMSsQE4rUqD"
      },
      "source": [
        "import collections\n",
        "import pandas as pd\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YP-kUhTjBka",
        "outputId": "c5aec2d2-94c8-47bd-d887-044a3bc9c5c2"
      },
      "source": [
        "device"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-D-6jdQkMXP"
      },
      "source": [
        "BATCH_SIZE = 16"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rusQRmGvsl4u"
      },
      "source": [
        "df = pd.read_csv('/content/heb-squad/data/final/heb-train-v2.0.csv')\n",
        "val_df = pd.read_csv('/content/heb-squad/data/final/heb-dev-v2.0.csv')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WC_Y9KL13MW"
      },
      "source": [
        "df = df[(df.answer_start_heb > 0) & (df.answer_end_heb > 0)]\n",
        "val_df = val_df[(val_df.answer_start_heb > 0) & (val_df.answer_end_heb > 0)]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw8vhXWnoAUN"
      },
      "source": [
        "Remove answer for impossible questions (The answer is actually the plausible answer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6lCO5qnn_YM"
      },
      "source": [
        "df['plausible_answer'] = df['answer']\n",
        "df['plausible_answer_start_heb'] = df['answer_start_heb']\n",
        "df['plausible_answer_end_heb'] = df['answer_end_heb']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNZTRt04qLtl"
      },
      "source": [
        "df.loc[df.is_impossible == 1, 'answer'] = ''\n",
        "df.loc[df.is_impossible == 1, 'answer_start_heb'] = 0\n",
        "df.loc[df.is_impossible == 1, 'answer_end_heb'] = 0"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "DvDsG3Qptzxa",
        "outputId": "d81c119f-8f38-4c3e-ce9c-91dd1b3b832c"
      },
      "source": [
        "df.sample(3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer_end</th>\n",
              "      <th>is_impossible</th>\n",
              "      <th>article</th>\n",
              "      <th>context_marked</th>\n",
              "      <th>answer_start_heb</th>\n",
              "      <th>answer_end_heb</th>\n",
              "      <th>plausible_answer</th>\n",
              "      <th>plausible_answer_start_heb</th>\n",
              "      <th>plausible_answer_end_heb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10170</th>\n",
              "      <td>56de4a81cffd8e1900b4b7b8</td>\n",
              "      <td>HDI של 0.8 או יותר נחשב לייצג \"פיתוח גבוה\". זה...</td>\n",
              "      <td>מהי המדינה הגדולה ביותר שנכללה בין שבע המדינות...</td>\n",
              "      <td>רוּסִיָה</td>\n",
              "      <td>474</td>\n",
              "      <td>480</td>\n",
              "      <td>0</td>\n",
              "      <td>Human_Development_Index</td>\n",
              "      <td>HDI של 0.8 או יותר נחשב לייצג \"פיתוח גבוה\". זה...</td>\n",
              "      <td>346</td>\n",
              "      <td>351</td>\n",
              "      <td>רוּסִיָה</td>\n",
              "      <td>346</td>\n",
              "      <td>351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23890</th>\n",
              "      <td>56f9453c9b226e1400dd12e4</td>\n",
              "      <td>קטע של רחוב מזרח 58th 40 ° 45′40.3 ″ N 73 ° 57...</td>\n",
              "      <td>מהו הקטע של רחוב מזרח 58 בין לקסינגטון לשדרות ...</td>\n",
              "      <td>דרך המעצבים</td>\n",
              "      <td>158</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>List_of_numbered_streets_in_Manhattan</td>\n",
              "      <td>קטע של רחוב מזרח 58th 40 ° 45′40.3 ″ N 73 ° 57...</td>\n",
              "      <td>148</td>\n",
              "      <td>159</td>\n",
              "      <td>דרך המעצבים</td>\n",
              "      <td>148</td>\n",
              "      <td>159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8339</th>\n",
              "      <td>56dde2609a695914005b964c</td>\n",
              "      <td>בין השנים 1590–1712 החזיקו ההולנדים גם באחד הי...</td>\n",
              "      <td>מה היה אחד מהכיבושים של הצי ההולנדי?</td>\n",
              "      <td>שבירת תחום ההשפעה הפורטוגזי על האוקיינוס ​​ההו...</td>\n",
              "      <td>143</td>\n",
              "      <td>224</td>\n",
              "      <td>0</td>\n",
              "      <td>Dutch_Republic</td>\n",
              "      <td>בין השנים 1590–1712 החזיקו ההולנדים גם באחד הי...</td>\n",
              "      <td>115</td>\n",
              "      <td>170</td>\n",
              "      <td>שבירת תחום ההשפעה הפורטוגזי על האוקיינוס ​​ההו...</td>\n",
              "      <td>115</td>\n",
              "      <td>170</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             id  ... plausible_answer_end_heb\n",
              "10170  56de4a81cffd8e1900b4b7b8  ...                      351\n",
              "23890  56f9453c9b226e1400dd12e4  ...                      159\n",
              "8339   56dde2609a695914005b964c  ...                      170\n",
              "\n",
              "[3 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmV4IjIFyRLp"
      },
      "source": [
        "## Transform the texts to encodings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMrfc6g40F4R"
      },
      "source": [
        "train_ids = df.id.to_list()\n",
        "train_contexts = df.context.to_list()\n",
        "train_questions = df.question.to_list()\n",
        "\n",
        "val_ids = val_df.id.to_list()\n",
        "val_contexts = val_df.context.to_list()\n",
        "val_questions = val_df.question.to_list()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mse3mOG46jz2"
      },
      "source": [
        "train_answers = df.apply(lambda x: {'answer_start' : x.answer_start_heb, 'answer_end': x.answer_end_heb}, axis=1).to_list()\n",
        "val_answers = val_df.apply(lambda x: {'answer_start' : x.answer_start_heb, 'answer_end': x.answer_end_heb}, axis=1).to_list()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn_xUWtEwZID"
      },
      "source": [
        "from transformers import BertTokenizerFast\n",
        "tokenizer = BertTokenizerFast.from_pretrained('onlplab/alephbert-base')\n",
        "\n",
        "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62IVlr9wyaF0"
      },
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i in range(len(answers)):\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "        if answers[i]['answer_end'] > 0:\n",
        "          end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
        "        else:\n",
        "          end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
        "\n",
        "        # if start position is None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "add_token_positions(train_encodings, train_answers)\n",
        "add_token_positions(val_encodings, val_answers)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUQhhBRI0L1K"
      },
      "source": [
        "train_encodings.update({'id': train_ids})\n",
        "val_encodings.update({'id': val_ids})"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfgYfUX49V9N"
      },
      "source": [
        "## Create PyTorch dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbUSlWHV31_b"
      },
      "source": [
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get all encoded vals as tensors\n",
        "        vals = {key: torch.tensor(val[idx]) for key, val in self.encodings.items() if key != 'id'}\n",
        "        # Add the id which is a string, used to map predictions to ids later\n",
        "        vals.update({key: val[idx] for key, val in self.encodings.items() if key == 'id'})\n",
        "        return vals\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = SquadDataset(train_encodings)\n",
        "val_dataset = SquadDataset(val_encodings)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peBZeD8czDey"
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia_AA0TS9dEh"
      },
      "source": [
        "## Import Hebrew BERT for Q&A fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70Qgzbis9Yhv",
        "outputId": "167804db-9fb0-4a1d-93a6-b63b1453f003"
      },
      "source": [
        "from transformers import BertForQuestionAnswering\n",
        "model = BertForQuestionAnswering.from_pretrained(\"onlplab/alephbert-base\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at onlplab/alephbert-base were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyf1U37pyPQ8"
      },
      "source": [
        "def load_checkpoint(path, model, optimizer):\n",
        "\n",
        "  # Load checkpoint from Google Drive\n",
        "  checkpoint = torch.load(path)\n",
        "\n",
        "  # Load checkpoint to model and optimizer\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "  checkpoint_epoch = checkpoint['epoch']\n",
        "\n",
        "  return checkpoint_epoch"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny8CODXqinn2"
      },
      "source": [
        "## Code to evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv1uwF39in76"
      },
      "source": [
        "def calc_metrics(model_inf, dataloader):\n",
        "  \"\"\"\n",
        "  Given a model and a dataset calculate the following metrics:\n",
        "  - Exact Match\n",
        "  - F1 Score\n",
        "  - Loss\n",
        "  @returns (F1, EM, Loss, EM Scores, F1 Scores)\n",
        "  \"\"\"\n",
        "\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  f1_scores = {}\n",
        "  exact_scores = {}\n",
        "\n",
        "  loss = 0\n",
        "  total_cnt = 0\n",
        "\n",
        "  for counter, batch in tqdm(enumerate(dataloader)):\n",
        "    with torch.no_grad():\n",
        "\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      start_positions = batch['start_positions'].to(device)\n",
        "      end_positions = batch['end_positions'].to(device)\n",
        "      ids = batch['id']\n",
        "      outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "      loss += outputs[0]\n",
        "\n",
        "      pred_start_positions = torch.argmax(outputs['start_logits'], axis=1)\n",
        "      pred_end_positions = torch.argmax(outputs['end_logits'], axis=1)\n",
        "\n",
        "      # Calc scores\n",
        "      for i in range(len(input_ids)):\n",
        "\n",
        "        total_cnt += 1\n",
        "\n",
        "        # Check if we have an exact match\n",
        "        if start_positions[i] == pred_start_positions[i] and end_positions[i] == pred_end_positions[i]:\n",
        "          # We have an exact match, mark it\n",
        "          exact_scores[ids[i]] = 1\n",
        "        else:\n",
        "          # No match, mark with 0 or keep previous marking\n",
        "          exact_scores[ids[i]] = max(0, exact_scores.get(ids[i], 0))\n",
        "\n",
        "        # Get the predicted answer token sequence\n",
        "        pred_tokens = input_ids[i][pred_start_positions[i]:pred_end_positions[i] + 1].tolist()\n",
        "        answer_tokens = input_ids[i][start_positions[i]:end_positions[i] + 1].tolist()\n",
        "\n",
        "        # Check how many predicted tokens correspond to gold answer tokens\n",
        "        common = collections.Counter(answer_tokens) & collections.Counter(pred_tokens)\n",
        "        num_same = sum(common.values())\n",
        "\n",
        "        # If none same, then F1=0 , else use the F1 formula\n",
        "        if num_same == 0:\n",
        "          score = 0\n",
        "        else:\n",
        "          precision = 1.0 * num_same / len(pred_tokens)\n",
        "          recall = 1.0 * num_same / len(answer_tokens)\n",
        "          score = (2 * precision * recall) / (precision + recall)\n",
        "        \n",
        "        # We take the max F1 score of the gold answers\n",
        "        f1_scores[ids[i]] = max(score, f1_scores.get(ids[i], 0))\n",
        "\n",
        "  f1 = sum(f1_scores.values()) / len(f1_scores)\n",
        "  em = sum(exact_scores.values()) / len(exact_scores)\n",
        "  epoch_avg_loss = loss / total_cnt\n",
        "\n",
        "  return (f1, em, epoch_avg_loss, exact_scores, f1_scores)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF58LfHezZIu"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c2bb3d95363048688c0962ba9fc57bfc",
            "0b4579d3c3f1479ebc09c9c1b8382abb",
            "b3f13e240ed140b3a98f729c6d932b94",
            "ac8100de07f2482f878c61e10a81ef39",
            "caaa2acfaab448b3aa9ff42166a735f3",
            "b934bfce3a804dbe9ae97554da9bfc89",
            "fe0b6e3ec1354f4f91e68ca23806d87e",
            "63629e10a88e429f82ce82ef613845f5",
            "1aebb6e2b899489a9bed56306fa63687",
            "8c15c6da8f7c4d619dc1473c28213cde",
            "e5576954f0e54e2dbe0dd19669a26827",
            "080479a8ee93416cbe8d0293907ba9bd",
            "83f9caa4792f4bb4b9d76d2d41073535",
            "36374d9b6b76494d8551a8d0545fff75",
            "935c8b070cd64a94961cf55008a9c31a",
            "599dde8bf87247ad9d2ee85ff7ab4482",
            "1a1777c285a549fe87d2a65453b49fc8",
            "9a9f475b56a942f6bcb0307765e47e58",
            "096870196dcc471b8ae95b73a538efd5",
            "7e724a7a570543988b7052525113ab95",
            "73d9846f1923405cb2ff9d82b97a89cf",
            "07f112bdfff2415d866596eb2deca0e9"
          ]
        },
        "id": "w6HfG_FE9hMh",
        "outputId": "b3e6fc61-e353-4992-f976-0a2d71b20fd0"
      },
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "checkpoint_path = '/content/drive/MyDrive/nlp/checkpoints/checkpoint_3_0.446_0.346_0.758_0.153'\n",
        "start_epoch = load_checkpoint(checkpoint_path, model, optim) + 1\n",
        "\n",
        "print(f'Training model from epoch={start_epoch}')\n",
        "\n",
        "for epoch in range(start_epoch, 6):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for counter, batch in enumerate(train_loader):\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        epoch_loss += loss\n",
        "\n",
        "        if counter % 250 == 0 and counter > 0:\n",
        "            print('Epoch %d, %d/%d, Current Loss = %.4f' % (epoch, counter, len(train_loader), loss))\n",
        "\n",
        "    avg_train_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    print('Epoch %d, Train Loss = %.4f' % (epoch, avg_train_loss))\n",
        "\n",
        "    f1, em, avg_val_loss, _, _ = calc_metrics(model, val_loader)\n",
        "\n",
        "    print('Epoch %d, Val Loss = %.4f, F1 = %.4f, EM=%.4f' % (epoch, avg_val_loss, f1, em))\n",
        "\n",
        "    train_loss.append(avg_train_loss)\n",
        "    val_loss.append(avg_val_loss)\n",
        "\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optim.state_dict(),\n",
        "        'f1': f1,\n",
        "        'em': em,\n",
        "        'avg_val_loss': avg_val_loss,\n",
        "        'avg_train_loss': avg_train_loss\n",
        "    }\n",
        "    savepath=f'/content/drive/MyDrive/nlp/checkpoints/checkpoint_{epoch}_{f1:.3f}_{em:.3f}_{avg_train_loss:.3f}_{avg_val_loss:.3f}'\n",
        "    torch.save(state, savepath)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model from epoch=4\n",
            "Epoch 4, 250/7113, Current Loss = 0.4844\n",
            "Epoch 4, 500/7113, Current Loss = 0.8463\n",
            "Epoch 4, 750/7113, Current Loss = 0.5753\n",
            "Epoch 4, 1000/7113, Current Loss = 0.4836\n",
            "Epoch 4, 1250/7113, Current Loss = 0.3497\n",
            "Epoch 4, 1500/7113, Current Loss = 0.3594\n",
            "Epoch 4, 1750/7113, Current Loss = 1.0043\n",
            "Epoch 4, 2000/7113, Current Loss = 0.6065\n",
            "Epoch 4, 2250/7113, Current Loss = 0.8762\n",
            "Epoch 4, 2500/7113, Current Loss = 0.3901\n",
            "Epoch 4, 2750/7113, Current Loss = 0.1339\n",
            "Epoch 4, 3000/7113, Current Loss = 0.7391\n",
            "Epoch 4, 3250/7113, Current Loss = 0.3710\n",
            "Epoch 4, 3500/7113, Current Loss = 0.6276\n",
            "Epoch 4, 3750/7113, Current Loss = 0.2371\n",
            "Epoch 4, 4000/7113, Current Loss = 1.1292\n",
            "Epoch 4, 4250/7113, Current Loss = 0.5109\n",
            "Epoch 4, 4500/7113, Current Loss = 1.1191\n",
            "Epoch 4, 4750/7113, Current Loss = 0.2917\n",
            "Epoch 4, 5000/7113, Current Loss = 0.4713\n",
            "Epoch 4, 5250/7113, Current Loss = 0.6357\n",
            "Epoch 4, 5500/7113, Current Loss = 0.4634\n",
            "Epoch 4, 5750/7113, Current Loss = 0.5398\n",
            "Epoch 4, 6000/7113, Current Loss = 0.6960\n",
            "Epoch 4, 6250/7113, Current Loss = 0.8120\n",
            "Epoch 4, 6500/7113, Current Loss = 0.2071\n",
            "Epoch 4, 6750/7113, Current Loss = 0.6755\n",
            "Epoch 4, 7000/7113, Current Loss = 0.8749\n",
            "Epoch 4, Train Loss = 0.6203\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2bb3d95363048688c0962ba9fc57bfc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4, Val Loss = 0.1675, F1 = 0.4767, EM=0.3762\n",
            "Epoch 5, 250/7113, Current Loss = 0.7076\n",
            "Epoch 5, 500/7113, Current Loss = 0.3364\n",
            "Epoch 5, 750/7113, Current Loss = 0.8248\n",
            "Epoch 5, 1000/7113, Current Loss = 0.2090\n",
            "Epoch 5, 1250/7113, Current Loss = 0.0538\n",
            "Epoch 5, 1500/7113, Current Loss = 0.6386\n",
            "Epoch 5, 1750/7113, Current Loss = 0.5536\n",
            "Epoch 5, 2000/7113, Current Loss = 0.7554\n",
            "Epoch 5, 2250/7113, Current Loss = 0.3633\n",
            "Epoch 5, 2500/7113, Current Loss = 0.6058\n",
            "Epoch 5, 2750/7113, Current Loss = 0.8002\n",
            "Epoch 5, 3000/7113, Current Loss = 0.3535\n",
            "Epoch 5, 3250/7113, Current Loss = 0.5457\n",
            "Epoch 5, 3500/7113, Current Loss = 0.8453\n",
            "Epoch 5, 3750/7113, Current Loss = 0.3671\n",
            "Epoch 5, 4000/7113, Current Loss = 0.4875\n",
            "Epoch 5, 4250/7113, Current Loss = 0.7399\n",
            "Epoch 5, 4500/7113, Current Loss = 0.9238\n",
            "Epoch 5, 4750/7113, Current Loss = 0.5459\n",
            "Epoch 5, 5000/7113, Current Loss = 0.3841\n",
            "Epoch 5, 5250/7113, Current Loss = 0.7498\n",
            "Epoch 5, 5500/7113, Current Loss = 0.3475\n",
            "Epoch 5, 5750/7113, Current Loss = 0.2761\n",
            "Epoch 5, 6000/7113, Current Loss = 0.5011\n",
            "Epoch 5, 6250/7113, Current Loss = 0.3145\n",
            "Epoch 5, 6500/7113, Current Loss = 0.3089\n",
            "Epoch 5, 6750/7113, Current Loss = 0.2016\n",
            "Epoch 5, 7000/7113, Current Loss = 0.7628\n",
            "Epoch 5, Train Loss = 0.5145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "080479a8ee93416cbe8d0293907ba9bd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5, Val Loss = 0.2075, F1 = 0.3976, EM=0.3093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4Evv7DluD4t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDASujBsuD9l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBDDB6UOuD_6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2EnaciBuECr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOc3_J2eFBQO"
      },
      "source": [
        "def map_id_to_gold_answers(df):\n",
        "  \"\"\"\n",
        "  In the SQuaD dev set there are multiple correct (gold) answers.\n",
        "  This methods creates a mapping between the question id to\n",
        "  its gold answers\n",
        "  \"\"\"\n",
        "  ids_to_gold = {}\n",
        "  ids_to_context = {}\n",
        "\n",
        "  # Group by id as validation set has several gold answer\n",
        "  for id, cols in df.groupby(by='id'):\n",
        "    gold_answers = [x[1].context[x[1].answer_start_heb:x[1].answer_end_heb] for x in cols.iterrows()]\n",
        "    context = cols.iloc[0].context\n",
        "\n",
        "    # TODO: Normalize answers!\n",
        "\n",
        "    ids_to_gold[id] = gold_answers\n",
        "    ids_to_context[id] = context\n",
        "  \n",
        "  return ids_to_gold, ids_to_context\n",
        "\n",
        "val_gold_answers, val_contexts = map_id_to_gold_answers(val_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN8OydXe2iPr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifBogV1oSOQ_"
      },
      "source": [
        "# model.to(device)\n",
        "\n",
        "# predictions = {}\n",
        "\n",
        "# for counter, batch in enumerate(val_loader):\n",
        "#   with torch.no_grad():\n",
        "  \n",
        "#     input_ids = batch['input_ids'].to(device)\n",
        "#     attention_mask = batch['attention_mask'].to(device)\n",
        "#     start_positions = batch['start_positions'].to(device)\n",
        "#     end_positions = batch['end_positions'].to(device)\n",
        "#     ids = batch['id']\n",
        "#     outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "    \n",
        "#     pred_start_positions = torch.argmax(outputs['start_logits'], axis=1)\n",
        "#     pred_end_positions = torch.argmax(outputs['end_logits'], axis=1)\n",
        "\n",
        "#     # print(pred_start_positions, pred_end_positions)\n",
        "\n",
        "#     for i in range(1):\n",
        "#       # context = val_contexts.get(ids[i])\n",
        "#       x = tokenizer.convert_ids_to_tokens(input_ids[i])\n",
        "#       # print(x[pred_start_positions[i]:pred_end_positions[i]+1])\n",
        "\n",
        "#       # Start with the first token.\n",
        "#       answer = x[pred_start_positions[i]]\n",
        "\n",
        "#       # Select the remaining answer tokens and join them with whitespace.\n",
        "#       for j in range(pred_start_positions[i] + 1, pred_end_positions[i] + 1):\n",
        "          \n",
        "#           # If it's a subword token, then recombine it with the previous token.\n",
        "#           if x[j][0:2] == '##':\n",
        "#               answer += x[j][2:]\n",
        "          \n",
        "#           # Otherwise, add a space then the token.\n",
        "#           else:\n",
        "#               answer += ' ' + x[j]\n",
        "#       predictions[ids[i]] = answer\n",
        "\n",
        "#       # start = val_encodings.token_to_chars(i, pred_start_positions[i])\n",
        "#       # end = val_encodings.token_to_chars(i, pred_end_positions[i])\n",
        "#       # pred_answer = context[start.start:end.end]\n",
        "#       # print(pred_answer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRSUQqmfoskN"
      },
      "source": [
        "# from tqdm.notebook import trange, tqdm\n",
        "\n",
        "# def calc_metrics(model_inf, dataloader):\n",
        "\n",
        "#   model.to(device)\n",
        "#   model.eval()\n",
        "\n",
        "#   # tp_start = 0\n",
        "#   # tp_end = 0\n",
        "\n",
        "#   f1_scores = {}\n",
        "#   exact_scores = {}\n",
        "\n",
        "#   loss = 0\n",
        "#   total_cnt = 0\n",
        "\n",
        "#   for counter, batch in tqdm(enumerate(dataloader)):\n",
        "#     with torch.no_grad():\n",
        "\n",
        "#       input_ids = batch['input_ids'].to(device)\n",
        "#       attention_mask = batch['attention_mask'].to(device)\n",
        "#       start_positions = batch['start_positions'].to(device)\n",
        "#       end_positions = batch['end_positions'].to(device)\n",
        "#       ids = batch['id']\n",
        "#       outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "#       loss += outputs[0]\n",
        "\n",
        "#       pred_start_positions = torch.argmax(outputs['start_logits'], axis=1)\n",
        "#       pred_end_positions = torch.argmax(outputs['end_logits'], axis=1)\n",
        "\n",
        "#       # # Array of True/False if pred is same as label\n",
        "#       # comp_start = start_positions == pred_start_positions\n",
        "#       # comp_end = end_positions == pred_end_positions\n",
        "#       # score = sum(comp_start.cpu().numpy() & comp_end.cpu().numpy())\n",
        "      \n",
        "#       # # We take the max score of the gold answers\n",
        "#       # exact_scores[ids[i]] = max(score, exact_scores.get(ids[i], 0))\n",
        "\n",
        "#       # # True = 1 and False = 0 therefore we count how many True\n",
        "#       # # tp_start += comp_start.count_nonzero()\n",
        "#       # # tp_end += comp_end.count_nonzero()\n",
        "\n",
        "#       # total_cnt += input_ids.size()[0]\n",
        "\n",
        "#       # Calc scores\n",
        "#       for i in range(len(input_ids)):\n",
        "\n",
        "#         total_cnt += 1\n",
        "\n",
        "#         # Check if we have an exact match\n",
        "#         if start_positions[i] == pred_start_positions[i] and end_positions[i] == pred_end_positions[i]:\n",
        "#           # We have an exact match, mark it\n",
        "#           exact_scores[ids[i]] = 1\n",
        "#         else:\n",
        "#           # No match, mark with 0 or keep previous marking\n",
        "#           exact_scores[ids[i]] = max(0, exact_scores.get(ids[i], 0))\n",
        "\n",
        "#         # Get the predicted answer token sequence\n",
        "#         pred_tokens = input_ids[i][pred_start_positions[i]:pred_end_positions[i] + 1].tolist()\n",
        "#         answer_tokens = input_ids[i][start_positions[i]:end_positions[i] + 1].tolist()\n",
        "\n",
        "#         # Check how many predicted tokens correspond to gold answer tokens\n",
        "#         common = collections.Counter(answer_tokens) & collections.Counter(pred_tokens)\n",
        "#         num_same = sum(common.values())\n",
        "\n",
        "#         # If none same, then F1=0 , else use the F1 formula\n",
        "#         if num_same == 0:\n",
        "#           score = 0\n",
        "#         else:\n",
        "#           precision = 1.0 * num_same / len(pred_tokens)\n",
        "#           recall = 1.0 * num_same / len(answer_tokens)\n",
        "#           score = (2 * precision * recall) / (precision + recall)\n",
        "        \n",
        "#         # We take the max F1 score of the gold answers\n",
        "#         f1_scores[ids[i]] = max(score, f1_scores.get(ids[i], 0))\n",
        "\n",
        "#   f1 = sum(f1_scores.values()) / len(f1_scores)\n",
        "#   em = sum(exact_scores.values()) / len(exact_scores)\n",
        "#   epoch_avg_loss = loss / total_cnt\n",
        "\n",
        "#   return (f1, em, epoch_avg_loss, exact_scores, f1_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kMxd9ZS896X"
      },
      "source": [
        "# for i in val_loader:\n",
        "#   batch = i\n",
        "#   break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg3T3t93S0Xf"
      },
      "source": [
        "## Calculate F1 score on dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpIkAy7RS7QO",
        "outputId": "5c890fdd-1ff4-4545-a749-9fad8a3ab887"
      },
      "source": [
        "def map_id_to_gold_answers(df):\n",
        "\n",
        "  # Group by id as validation set has several gold answer\n",
        "  for id, cols in df.groupby(by='id'):\n",
        "    # for row in zip(cols.answer_start_heb, cols.answer_end_heb):\n",
        "      # print(row.answer_start_heb, row.answer_end_heb)\n",
        "    gold_answers = [x[1].context[x[1].answer_start_heb:x[1].answer_end_heb] for x in cols.iterrows()]\n",
        "    #gold_answers = [x for x in zip(cols.answer_start_heb, cols.answer_end_heb)]\n",
        "    print(id, gold_answers)\n",
        "    if i == 10:\n",
        "      break\n",
        "    print()\n",
        "    i+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37. 5727d1953acd2414000ded  ['צחיח למחצה']\n",
            "\n",
            "38. 5727d1953acd2414000ded38  ['לוטון']\n",
            "\n",
            "38. 573459cbacc1501500babe38  [\"צ'סטרפילד קאונטי\"]\n",
            "\n",
            "56  ['קרדינל', '23:00', '1973']\n",
            "\n",
            "56 dec8773277331400b4d730  ['פיטר פילז']\n",
            "\n",
            "56 דצמבר 1673277331400b4d70d  ['']\n",
            "\n",
            "56 דצמבר 2483277331400b4d711  [\"ג'ון מקיין\"]\n",
            "\n",
            "56 דצמבר 2483277331400b4d712  [\"רודי ג'וליאני\"]\n",
            "\n",
            "56 דצמבר 2483277331400b4d713  ['הסביבה והכלכלה']\n",
            "\n",
            "56 דצמבר 4913277331400b4d720  ['שש']\n",
            "\n",
            "56 דצמבר 4913277331400b4d721  [\"לוס אנג'לס טיימס\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1_hCeahb5m-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}